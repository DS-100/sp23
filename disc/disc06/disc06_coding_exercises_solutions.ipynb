{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry of Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1)** \n",
    "\n",
    "Suppose we have a dataset represented with the design matrix $\\text{span}(\\mathbb{X})$ and response vector $\\mathbb{Y}$. We use linear regression to solve for this and obtain optimal weights as $\\hat{\\theta}$. Label the following terms on the geometric interpretation of ordinary least squares:\n",
    "\n",
    "<img src = \"images/blank.jpg\"></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: See physical sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2)** \n",
    "Using the geometry of least squares, let’s answer a few questions about Ordinary Least Squares (OLS)!\n",
    "\n",
    "\n",
    "**a)** Which of the following are true about the optimal solution $\\hat{\\theta}$ to OLS? Recall that the least squares estimate $\\hat{\\theta}$ solves the normal equation $(\\Bbb{X}^T\\Bbb{X})\\theta = \\Bbb{X}^T\\Bbb{Y}$.\n",
    "\n",
    "\n",
    "A. Using the normal equation, we can derive an optimal solution for simple linear regression with an $L_2$ loss.\n",
    "\n",
    "B. Using the normal equation, we can derive an optimal solution for simple linear regression with an $L_1$ loss.\n",
    "\n",
    "C. Using the normal equation, we can derive an optimal solution for a constant model with an $L_2$ loss.\n",
    "\n",
    "D. Using the normal equation, we can derive an optimal solution for a constant model with an $L_1$ loss.\n",
    "\n",
    "E. Using the normal equation, we can derive an optimal solution for the model $\\hat{y} = \\theta_1 x + \\theta_2 \\sin(x^2)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: A, C, E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Which of the following conditions are required for the least squares estimate in the previous subpart?\n",
    "\n",
    "A) $\\Bbb{X}$ must be full column rank.\n",
    "\n",
    "B) $\\Bbb{Y}$ must be full column rank.\n",
    "\n",
    "C) $\\Bbb{X}$ must be invertible.\n",
    "\n",
    "D) $\\Bbb{X^T}$ must be invertible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** What is always true about the residuals in the least squares regression? Select all that apply.\n",
    "\n",
    "A) They are orthogonal to the column space of the design matrix.\n",
    "\n",
    "B) They represent the errors of the predictions.\n",
    "\n",
    "C) Their sum is equal to the mean squared error.\n",
    "\n",
    "D) Their sum is equal to zero.\n",
    "\n",
    "E) None of the above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Which are true about the predictions made by OLS? Select all that apply.\n",
    "\n",
    "A) They are projections of the observations onto the column space of the design matrix.\n",
    "\n",
    "B) They are linear combinations of the features.\n",
    "\n",
    "C) They are orthogonal to the residuals.\n",
    "\n",
    "D) They are orthogonal to the column space of the features.\n",
    "\n",
    "E) None of the above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: A,B, C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** We fit a simple linear regression to our data $(x_i, y_i)$ for $i \\in \\{1, 2, \\dots, n\\}$, where $n$ is the number of samples, $x_i$ is the independent variable, and $y_i$ is the dependent variable. Our regression line is of the form $\\hat{y} = \\hat{\\theta_0} + \\hat{\\theta_1}x$. Suppose we plot the relationship between the residuals of the model and the $\\hat{y}_i$'s and find that there is a curve. What does this tell us about our model?\n",
    "\n",
    "A)  The relationship between our dependent and independent variables is well represented by a line.\n",
    "\n",
    "B) The accuracy of the regression line varies with the size of the dependent variable.\n",
    "\n",
    "C) The variables need to be transformed, or additional independent variables are needed.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Which are the following is true of the mystery quantity $\\vec{v} = (I - \\Bbb{X}(\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T) \\Bbb{Y}$?\n",
    "\n",
    "A) The vector $\\vec{v}$ represents the residuals for any linear model.\n",
    "\n",
    "B) If the $\\Bbb{X}$ matrix contains the $\\vec{1}$ vector, then the sum of the elements in vector $\\vec{v}$ is 0 (i.e. $\\sum_i v_i = 0$).\n",
    "\n",
    "C) All the column vectors $x_i$ of $\\Bbb{X}$ are orthogonal to $\\vec{v}$.\n",
    "\n",
    "D) If $\\Bbb{X}$ is of shape $n$ by $p$, there are $p$ elements in vector $\\vec{v}$.\n",
    "\n",
    "E) For any $\\alpha$, $\\Bbb{X}\\alpha$ is orthogonal to $\\vec{v}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: B, C, E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** Derive the least squares estimate $\\hat{\\theta}$ by leveraging the geometry of least squares. \n",
    "\n",
    "*Note:* While this isn't a \"proof\" or \"derivation\" class (and you certainly will not be asked to derive anything of this sort on an exam), we believe that understanding the geometry of least squares enough to derive the least squares estimate shows great understanding of all the linear regression concepts we want you to know! Additionally, it provides great practice with tricky linear algebra concepts such as rank, span, orthogonality, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: See physical sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driving with a Constant Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is trying to use modeling to drive his car autonomously. To do this, he collects a lot of data where he drives around his neighborhood, and he wants your help to design a model that can drive on his behalf in the future using the outputs of the models you design. We will tackle two aspects of this autonomous car modeling framework: going forward and turning.\n",
    "\n",
    "We show some statistics from the collected dataset below using *pd.describe*, which returns the  mean, standard deviation, quartiles, minimum, and maximum for the two columns in the dataset: *target_speed* and *degree_turn*.\n",
    "\n",
    "<img src=\"images/describe.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Suppose the model predicts the target speed of the car. Using constant models trained on the speeds of the collected data shown above with $L_1$ and $L_2$ loss functions, which of the following is true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. The model trained with the $L_1$ loss will always drive slower than the model trained with $L_2$ loss.\n",
    "\n",
    "B. The model trained with the $L_2$ loss will always drive slower than the model trained with $L_1$ loss.\n",
    "\n",
    "C. The model trained with the $L_1$ loss will sometimes drive slower than the model trained with $L_2$ loss.\n",
    "\n",
    "D. The model trained with the $L_2$ loss will sometimes drive slower than the model trained with $L_1$ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3a = \"A\"\n",
    "q3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Finding that the model trained with the $L_2$ loss drives too slowly, Adam changes the loss function for the constant model where the loss is penalized \\textbf{more} if the speed is higher. That way, the model wants to optimize more for the case where we wish to drive faster since the loss is higher, accomplishing his goal. Adam writes this as $L(y, \\hat{y}) = y(y - \\hat{y})^2$.\n",
    "\n",
    "Find the optimal $\\hat{\\theta}$ for the constant model using the new empirical risk function $R(\\theta)$ below:\n",
    "\n",
    "$$\n",
    "R(\\theta) = \\frac{1}{n} \\sum_i y_i (y_i - \\theta)^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: See physical sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Suppose he is working on a model that predicts the degree of turning at a particular time between 0 and 359 degrees using the data in the *degree_turn* column. Explain why a constant model is likely inappropriate in this use case.\n",
    "\n",
    "*Extra:* If you've studied some physics, you may recognize the behavior of our constant model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any constant model will essentially be always turning at an angle and will\n",
    "be unable to turn either direction or go straight (i.e. it’ll essentially go in a circle\n",
    "forever).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Suppose we finally expand our modeling framework to use simple linear regression (i.e. $f_\\theta(x) = \\theta_{w,0} + \\theta_{w,1}x$). For our first simple linear regression model, we predict the turn angle ($y$) using target speed ($x$). Our optimal parameters are: $\\hat{\\theta}_{w,1} = 0.019$ and $\\hat{\\theta}_{w,0} = 143.1$.\n",
    "\n",
    "However, we realize that we actually want a model that predicts target speed (our new $y$) using turn angle, our new $x$ (instead of the other way around)! What are our new optimal parameters for this new model? ($\\textit{Hint: use the information in the table.}$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\hat{\\theta}_1 = \\frac{r\\sigma_{\\text{speed}}}{\\sigma_{\\text{turn}}}$. Currently, we have a quantity $\\hat{\\theta}_{w,1} = \\frac{r \\sigma_{\\text{turn}}}{\\sigma_{\\text{speed}}}$. Then, $\\hat{\\theta}_1 = \\hat{\\theta}_{w,1} \\frac{\\sigma^2_{\\text{speed}}}{\\sigma^2_{\\text{turn}}} = 0.019 * \\frac{46.678744^2}{153.641504^2} = 0.00175$.\n",
    "\n",
    "Then, $\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1 \\bar{x} = 32.92 - 0.00175 * 143.72 = 32.67$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling using Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to model exam grades for $\\textit{Data 100}$ students. We collect various information about student habits, such as how many hours they studied, how many hours they slept before the exam, and how many lectures they attended, and observe how well they did on the exam. Suppose you collect such information on $n$ students and wish to use a multiple-regression model to predict exam grades.\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "         1&study_1 & sleep_1 & lectures_1 \\\\\n",
    "         1&...&... &... & \\\\\n",
    "         1&...&... &... & \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Suppose on our $n$ individuals, we construct our design matrix $\\mathbb{X}$, adding an $\\textbf{intercept term}$, and use the OLS formula to obtain the following $\\hat{\\theta}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{\\theta} = \\begin{bmatrix} 0.5 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Suppose our design matrix $\\mathbb{X}$ was constructed such that the first is the bias, the second column represents how many hours each of the $n$ students studied, the third contains how many hours each student slept before the exam, and the fourth represents how many lectures each student attended. With this knowledge, give an interpretation of what each entry of $\\hat{\\theta}$ means in context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each regression coefficient (a component of the $\\theta$ vector), except for $\\theta_0$,  represents the amount we expect an individual's exam score to go up when increasing the corresponding variable by one unit and holding all other variables fixed. For instance, for the first component, when holding `hours of sleep` and `lectures attended` constant, an individual's score is expected to go up by 3 per extra hour spent studying. The other components can be interpreted similarly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** After fitting this model, suppose we have two individuals for which we would like to predict their exam grades using these variables. Suppose Individual 1, slept 10 hours, studied 15 hours, and attended 4 lectures. Suppose also Individual 2, slept 5 hours, studied 20 hours, and attended 10 lectures. Construct a matrix $\\mathbb{X}'$ such that, if you computed $\\mathbb{X}'\\hat{\\theta}$,\n",
    "you would obtain a vector of each individual's predicted exam scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\mathbb{X}' = \\begin{bmatrix} 1 & 15 & 10 & 4 \\\\ 1 & 20 & 5 & 10 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Denote $\\mathbb{Y}'$ as a $2 \\times 1$ vector that represents the actual exam scores of the individuals we are predicting on. Write out an expression that evaluates to the MSE of our predictions, written as a function of the squared L2-norm of a vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{MSE} = \\frac{1}{2} \\| \\mathbb{Y}' - \\mathbb{X}'\\hat{\\theta} \\|^{2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aea83e800b3f611b156506e3451d6446f84c720051bc83a1d66a677713fe7b36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
