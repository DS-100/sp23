{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab07.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 7: Feature Engineering and Gradient Descent\n",
    "\n",
    "In this lab, we will work through the process of:\n",
    "1. Defining loss functions,\n",
    "1. Performing feature engineering,\n",
    "1. Minimizing loss functions using numeric methods and analytical methods,\n",
    "1. Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features,\n",
    "1. Computing a gradient for a nonlinear model, and\n",
    "1. Using gradient descent to optimize the nonline model.\n",
    "\n",
    "This lab will continue using the toy `tips` calculation dataset used in a prior lab.\n",
    "\n",
    "**The on-time deadline is Tuesday, March 7th, 11:59pm. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** Some part of the video is recorded in Spring 2022. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but content is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"LohVOmiulHQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *List names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "To begin, let's load the tips dataset from the `seaborn` library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. As earlier, we'll be trying to predict tips from the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Functions\n",
    "\n",
    "So far, we've only considered models of the form $\\hat{y} = f_{\\theta}(x) = \\theta_0 + \\sum_{j=1}^p x_j\\theta_j$, where $\\hat{y}$ is quantitative continuous. \n",
    "\n",
    "We call this a linear model because it is a linear combination of the features $x_1, \\dots, x_p$. However, our features don't need to be numbers: we could have categorical values such as names. Additionally, the true relationship doesn't have to be linear, as we could have a relationship that is quadratic, such as the relationship between the height of a projectile and time.\n",
    "\n",
    "In these cases, we often apply **feature functions**, functions that take in some value and output another value. This might look like converting a string into a number, combining multiple numeric values, or creating a boolean value from some filter.\n",
    "\n",
    "Then, if we call $\\phi$ (\"phi\") our \"phi\"-ture function, our model takes the form $\\hat{y} = f_{\\theta}(x) = \\theta_0 + \\sum_{j=1}^p \\phi(x)_j\\theta_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Feature Functions\n",
    "\n",
    "1. **One-hot encoding**\n",
    "    - Converts a single categorical feature into many binary features, each of which represents one of the possible values in the original column.\n",
    "    - Each of the binary feature columns produced contains a 1 for rows that had that column's label in the original column, and 0 elsewhere.\n",
    "1. **Polynomial feature**\n",
    "    - Creates polynomial combinations of features.\n",
    "1. **Normalized/Standarized feature**\n",
    "    - Normalizes features so they have mean of 0 and standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Defining the Model and Engineering Features\n",
    "\n",
    "In Lab 5, we used Simple Linear Regression (SLR) and constant model on this dataset. Now let's make a more complicated model that utilizes other features in our dataset. You can imagine that we might want to use the features with an equation that looks as shown below:\n",
    "\n",
    "$$ \\text{Tip} = \\theta_0 + \\theta_1 \\cdot \\text{total}\\_\\text{bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size} $$\n",
    "\n",
    "Unfortunately, that's not possible because some of these features like \"day\" are not numbers, so it doesn't make sense to multiply by a numerical parameter. Let's start by converting some of these non-numerical values into numerical values.\n",
    "\n",
    "Before we do this, let's separate out the tips and the features into two separate variables, and add a bias term using `pd.insert` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.insert.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = data['tip']\n",
    "X = data.drop(columns='tip')\n",
    "X.insert(0, 'bias', 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1: Feature Engineering\n",
    "\n",
    "First, let's convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. \n",
    "\n",
    "For example, we could convert the `day` feature to a numerical value from 1-7. However, one of the disadvantages to directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can (and likely will) lower the performance of our model.\n",
    "\n",
    "Instead, let's use **one-hot encoding** to better represent these features!  As you learned in lecture, one-hot encoding is a feature engineering method that represents non-numeric features using boolean vectors (numerical values 0 or 1).\n",
    "\n",
    "In the `tips` dataset for example, we encode Sunday as the row vector `[0 0 0 1]` because our dataset only contains bills from Thursday through Sunday. This replaces the `day` feature with four boolean features indicating if the record occurred on Thursday, Friday, Saturday, or Sunday. One-hot encoding therefore assigns a more even weight across each category in non-numeric features.\n",
    "\n",
    "Complete the code below to one-hot encode our dataset. This dataframe holds our \"featurized\" data, which is also often denoted by $\\phi$.\n",
    "\n",
    "**Hint:** You should use sklearn's `OneHotEncoder` class ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) when doing your one-hot encoding. Note that `OneHotEncoder` transforms data into a SciPy sparse matrix; check out `.toarray()` ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html)) for how to convert this to a NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "\n",
    "    \n",
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "\n",
    "### Tutorial: fit()/predict()\n",
    "\n",
    "Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 13 features instead of 7 (including bias). Therefore, our linear model now similar to the below (note the order of thetas below does not necessarily match the order in the DataFrame):\n",
    "\n",
    "\\begin{array}\n",
    "\\text{Tip} & = \\theta_0 + \\theta_1 \\cdot \\text{size} + \\theta_2 \\cdot \\text{total}\\_\\text{bill} \\\\\n",
    "& + \\theta_3 \\cdot \\text{day}\\_\\text{Thur} + \\theta_4 \\cdot \\text{day}\\_\\text{Fri} + \\theta_5 \\cdot \\text{day}\\_\\text{Sat} + \\theta_6 \\cdot \\text{day}\\_\\text{Sun} \\\\\n",
    "& + \\theta_7 \\cdot \\text{sex}\\_\\text{Female} + \\theta_8 \\cdot \\text{sex}\\_\\text{Male} + \\theta_9 \\cdot \\text{smoker}\\_\\text{Yes} + \\theta_{10} \\cdot \\text{smoker}\\_\\text{No} + \\theta_{11} \\cdot \\text{time}\\_\\text{Lunch} \\\\\n",
    "&+ \\theta_{12} \\cdot \\text{time}\\_\\text{Dinner}\n",
    "\\end{array}\n",
    "\n",
    "We can represent the linear combination above as a matrix-vector product. Below, we practice syntax similar to the sklearn pipeline using a `MyZeroLinearModel` class with two methods, `predict` and `fit`.\n",
    "* `fit`: Compute parameters theta given data `X` and `y` and the underlying model.\n",
    "* `predict`: Compute estimate $\\hat{y}$ given the underlying model.\n",
    "\n",
    "When fitted, this model fails to do anything useful, setting all of its 13 parameters to zero. If you are unfamiliar with using python objects, please review [object oriented programming](https://cs61a.org/study-guide/objects/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyZeroLinearModel():    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        number_of_features = X.shape[1]\n",
    "        self._thetas = np.zeros(shape=(number_of_features, 1))\n",
    "\n",
    "# Running the code below produces all-zero thetas\n",
    "model0 = MyZeroLinearModel()\n",
    "model0.fit(one_hot_X, tips)\n",
    "model0._thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Fitting a Linear Model Using Numerical Methods\n",
    "\n",
    "\n",
    "The best fit model is determined by our loss function. Recall in Lab 5 and in Lecture 11 we defined multiple loss functions and found the optimal theta using the `scipy.optimize.minimize` function. \n",
    "\n",
    "\n",
    "In this question, we'll wrap this function into a method `fit()` in our class `MyScipyLinearModel`.\n",
    "To allow for different loss function, we create a `loss_function` parameter where the model can be fit accordingly. Example loss function are given as `l1` and `l2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Question 2a: scipy\n",
    "\n",
    "Complete the code below using `scipy.optimize.minimize`. Find and store the optimal thetas in the instance attribute `self._thetas`.\n",
    "\n",
    "**Hint:**\n",
    "* The starting guess should be some arbitrary array of the correct length. You may find `number_of_features` helpful.\n",
    "\n",
    "**Notes:**\n",
    "* Notice that `l1` and `l2` return term-wise loss and only accept observed and predicted y. We added a lambda function to help convert them into the right format for `scipy.optimize.minimize`.\n",
    "* Notice above that we extract the `'x'` entry in the dictionary returned by `minimize`. This entry corresponds to the optimal $\\theta$ estimated by the function, and it is the format that `minimize` uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l1(y, y_hat):\n",
    "    return np.abs(y - y_hat)\n",
    "\n",
    "def l2(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "class MyScipyLinearModel():    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "    \n",
    "    def fit(self, loss_function, X, y):\n",
    "        \"\"\"\n",
    "        Estimated optimal _thetas for the given loss function, \n",
    "        feature matrix X, and observations y. Store them in _thetas.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        loss_function: a function that takes in observed and predicted y, \n",
    "                       and return the loss calculated for each data point\n",
    "        X: a 2D dataframe (or numpy array) of numeric features\n",
    "        y: a 1D numpy array or Series of the dependent variable\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        number_of_features = X.shape[1]\n",
    "        starting_guess = ...\n",
    "        self._thetas = minimize(lambda theta:\n",
    "                                ...\n",
    "                                , x0 = starting_guess)['x']        \n",
    "        \n",
    "# Running the code below should produce some non-zero thetas.\n",
    "model = MyScipyLinearModel()\n",
    "model.fit(l2, one_hot_X, tips)\n",
    "print(\"L2 loss thetas:\")\n",
    "print(model._thetas)\n",
    "\n",
    "# Create a new model and fit the data again using l1 loss, it should should produce some non-zero thetas as well\n",
    "model_l1 = MyScipyLinearModel()\n",
    "model_l1.fit(l1, one_hot_X, tips)\n",
    "print(\"L1 loss thetas:\")\n",
    "print(model._thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for your model above should be just slightly larger than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to calculate the mean square error of the above model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"L2 loss MSE scipy: \" + str(mean_squared_error(model.predict(one_hot_X), tips)))\n",
    "print(\"L1 loss MSE scipy: \" + str(mean_squared_error(model_l1.predict(one_hot_X), tips)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b: sklearn\n",
    "\n",
    "Another way to fit a linear regression model is to use `scikit-learn`/`sklearn` as we have seen in Lab 6.  As a reminder, here are the three steps to use `sklearn`:\n",
    "\n",
    "1. Create an `sklearn` object.\n",
    "1. `fit` the object to data.\n",
    "1. Analyze fit, or call `predict`.\n",
    "\n",
    "\n",
    "The sklearn `LinearRegression` object ([documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)) which models the ordinary least squares problem, also using numerical methods to estimate $\\theta$'s. Fill in the code below such that `sklearn_model` fits OLS using sklearn.\n",
    "\n",
    "**Hint:** Since we have included the bias column in our design matrix explicitly, we need to adjust the `fit_intercept` parameter appropriately when creating the `LinearRegression` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sklearn_model = ...\n",
    "...\n",
    "print(\"sklearn with bias column thetas:\")\n",
    "print(sklearn_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    " \n",
    "---\n",
    " \n",
    "### Question 2c: sklearn and `fit_intercept`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid always explicitly building in a bias column into our design matrix, sklearn's `LinearRegression` object also supports `fit_intercept=True` during instantiation. \n",
    "\n",
    "Fill in the code below by first assigning `one_hot_X_nobias` to the `one_hot_X` design matrix with the bias column dropped, then fit a new `LinearRegression` model, with intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot_X_nobias = ...\n",
    "\n",
    "sklearn_model_intercept = ...\n",
    "...\n",
    "\n",
    "# Note that sklearn return intercept (theta_0) and coefficients (other thetas) separately\n",
    "# We concatenate the intercept and other thetas before printing for easier comparison with the models above\n",
    "print(\"sklearn with intercept thetas:\")\n",
    "print(np.concatenate(([sklearn_model_intercept.intercept_], sklearn_model_intercept.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printed the MSE for the SciPy and both sklearn solutions below (all using L2 loss). Notice that while the coefficients are different for the two sklearn models (with the bias column, vs. with `fit_intercept=True`), all three models have similar MSEs! We will explain this when we explore Gradient Descent later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE scipy: \\t\\t\\t\" + str(mean_squared_error(model.predict(one_hot_X), tips)))\n",
    "print(\"MSE sklearn bias column: \\t\" + str(mean_squared_error(sklearn_model.predict(one_hot_X), tips)))\n",
    "print(\"MSE sklearn intercept model: \\t\" + str(mean_squared_error(sklearn_model_intercept.predict(one_hot_X_nobias), tips)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Fitting the Model Using Analytic Methods\n",
    "\n",
    "Let's also fit our model analytically for the L2 loss function. Recall from lecture that with a linear model, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n}||\\Bbb{X}\\theta - \\Bbb{y}||^2$$\n",
    "\n",
    "We showed in lecture that the optimal $\\hat{\\theta}$ when $X^TX$ is invertible is given by the equation: $(X^TX)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3a: Analytic Solution Using Explicit Inverses\n",
    "\n",
    "For this problem, implement the analytic solution above using `np.linalg.inv` to compute the inverse of $X^TX$.\n",
    "\n",
    "**Hint**: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyAnalyticallyFitOLSModel():    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Sets _thetas using the analytical solution to the ordinary least squares problem\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X: a 2D dataframe (or numpy array) of numeric features (one-hot encoded)\n",
    "        y: a 1D numpy array or Series of the dependent variable\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to find the analytical solution for the `tips` dataset. Depending on the machine that you run your code on, you should either see a singular matrix error or end up with thetas that are nonsensical (magnitudes greater than $10^{15}$). This is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed.\n",
    "# The try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    model_analytical = MyAnalyticallyFitOLSModel()\n",
    "    model_analytical.fit(one_hot_X, tips)\n",
    "    analytical_thetas = model_analytical._thetas\n",
    "    print(analytical_thetas)\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, explain why we got the error above when trying to calculate the analytical solution for our one-hot encoded `tips` dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3c: Fixing Our One-Hot Encoding\n",
    "\n",
    "Now, let's modify our one-hot encoding approach from early so we don't get the error we saw in the previous part. Complete the code below to one-hot-encode our dataset such that `one_hot_X_revised` has no redundant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_revised(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data, removing redundancies.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features without any redundancies.\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "\n",
    "one_hot_X_revised = one_hot_encode_revised(X)\n",
    "display(one_hot_X_revised.head())\n",
    "    \n",
    "scipy_model = MyScipyLinearModel()\n",
    "scipy_model.fit(l2, one_hot_X_revised, tips)\n",
    "    \n",
    "analytical_model = MyAnalyticallyFitOLSModel()\n",
    "analytical_model.fit(one_hot_X_revised, tips)\n",
    "\n",
    "print(\"Our scipy numerical model's loss is: \", mean_squared_error(scipy_model.predict(one_hot_X_revised), tips))\n",
    "print(\"Our analytical model's loss is: \", mean_squared_error(analytical_model.predict(one_hot_X_revised), tips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the rank of the matrix using the NumPy function `np.linalg.matrix_rank`. We have printed the rank of the data and number of columns for you below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one_hot_X: \\n\"\n",
    "      + \"\\t number of columns: \" + str(len(one_hot_X.columns)) \\\n",
    "      + \"\\trank: \" + str(np.linalg.matrix_rank(one_hot_X)))\n",
    "print(\"one_hot_X_revised: \\n\"\n",
    "      + \"\\t number of columns: \" + str(len(one_hot_X_revised.columns)) \\\n",
    "      + \"\\trank: \" + str(np.linalg.matrix_rank(one_hot_X_revised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3d: Analyzing our new One-Hot Encoding\n",
    "\n",
    "Why did removing redundancies in our one-hot encoding fix the problem we had in 3a?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Having linearly dependent columns means our system of linear equations is underdetermined, i.e. there are multiple solution. There is not unique solution hence there is no inverse. An alternate approach is to use `np.linalg.pinv` or `np.linalg.solve` instead of `np.linalg.inv`. They returned **a** solution among the many possible solution. Even with the redundant features, `np.linalg.solve` `np.linalg.pinv`  will work for the example above as a result. However, in general, it's best to drop redundant features for inference purposes and we will explore this further in Homework 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "You may be wondering why `scipy.optimize.minimize` or `sklearn.LinearRegression` works as well with redundant features when an analytical solution fails above. This is because `scipy.optimize.minimize` and `sklearn.LinearRegression` use numerical optimization technique to find **a** solution---it does not require a unique solution through matrix inverses. In fact, the coefficients for sklearn model with bias column and sklearn with intercept model are very different (Q2b vs Q2c), but both give the same minimum MSE. Below, we explore a numerical optimization method called **gradient descent**. It is a simpler version of the default solver used by `scipy.optimize.minimize` and `sklearn.LinearRegression`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Sinusoidal Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data for this problem.\n",
    "df = pd.read_csv(\"lab7_data.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot this data, we see that there is a clear sinusoidal relationship between x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.scatter(df, x = \"x\", y = \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll show gradient descent is so powerful it can even optimize a nonlinear model (when the analytical solution is hard to derive). Specifically, we're going to model the relationship of our data by\n",
    "\n",
    "$$\\Large{\n",
    "f_{\\boldsymbol{\\theta}}(x) = \\theta_1x + \\sin(\\theta_2x)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is parameterized by both $\\theta_1$ and $\\theta_2$, which we can represent in the vector, $\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$.\n",
    "\n",
    "Note that a general sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. \n",
    "Here, we're assuming the amplitude $a$ is around 1, and the phase shifting parameter $c$ is around zero. We do not attempt to justify this assumption and you're welcome to see what happens if you ignore this assumption at the end of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that predicts $\\mathbb{Y}$ (the $y$-values) using $\\mathbb{X}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2\n",
    "    \"\"\"\n",
    "    theta_1 = theta[0]\n",
    "    theta_2 = theta[1]\n",
    "    return theta_1 * x + np.sin(theta_2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask---Why we don't just represent this as a linear model with a sinusoidal feature, just like we did earlier? The issue is that the theta is INSIDE the sine function, and hence this formulation is **nonlinear**. In other words, linear models use their parameters to adjust the scale of each feature, but $\\theta_2$ in this model adjusts the frequency of the feature. There are tricks we could play to use our linear model framework here, but we won't attempt this in our lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology: Loss\n",
    "\n",
    "We use the word \"loss\" in two different (but very related) contexts in this course.\n",
    "* In general, loss is the cost function that measures how far off model's prediction(s) is(are) from the actual value(s).\n",
    "    * **Per-datapoint loss** is a cost function that measures the cost of $y$ vs $\\hat{y}$ for a particular datapoint. For example, $L(y, \\hat{y}) = (y - \\hat{y})^2$ is the L2 error of the observed and predicted pair ($y$, $\\hat{y}$).\n",
    "    * **Loss** (without any adjectives) is generally a cost function measured across all datapoints. We often use this term interchangebly with **empirical risk** to denote the average per-datapoint loss. For example, MSE can be denoted as $MSE(\\theta) = L(\\theta) = R(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$\n",
    "* We prioritize using the latter definition, because we don't particularly look at a given datapoint's loss when optimizing a model. In other words, the dataset-level loss is the **objective function** that we'd like to minimize.\n",
    "    * Example: \"gradient of L2 loss\" means gradient of Mean Squared Error, not per-datapoint L2 loss.\n",
    "* On graded work we will be clearer about the distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4a: Computing the Gradient of the MSE With Respect to Theta on the Sin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall $\\hat{\\theta}$ is the value of $\\theta$ that minimizes our loss function. One way to solve for $\\hat{\\theta}$ is by computing the gradient of our loss function with respect to $\\theta$, like we did in lecture. Recall that the gradient is a column vector of two partial derivatives.\n",
    "\n",
    "**Task**: Write/derive the expressions for the following values and use them to fill in the functions below.\n",
    "\n",
    "* $R(\\theta)$ as `sin_mse`: our loss function, mean squared error, where `theta` represents $\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$. \n",
    "Recall that $$R(\\vec{\\theta}, \\mathbb{X}, \\vec{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.$$\n",
    "* $\\frac{\\partial R }{\\partial \\theta_1}$ as `sin_MSE_dt1`: the partial derivative of $R$ with respect to $\\theta_1$.\n",
    "* $\\frac{\\partial R }{\\partial \\theta_2}$ as `sin_MSE_dt2`: the partial derivative of $R$ with respect to $\\theta_2$.\n",
    "* We have completed for you `sin_MSE_gradient`, which computes $\\nabla_\\vec{\\theta} R(\\vec{\\theta}, \\mathbb{X}, \\vec{y})$  by calling `dt1` and `dt2` and returns the gradient `dt`.\n",
    "\n",
    "**Notes:**\n",
    "* We are still working with the DataFrame `df`.\n",
    "* To keep our code a more concise, use `np.mean` instead of taking `np.sum` then dividing by the length of the numpy array.\n",
    "* Another way to keep your code more concise is to use the function `sin_model` we defined which computes the output of the model.\n",
    "* Review the \"Terminology: Loss\" section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sin_MSE(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the l2 loss of our sinusoidal model given theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "def sin_MSE_dt1(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "def sin_MSE_dt2(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt.\n",
    "# It is already implemented for you.\n",
    "def sin_MSE_gradient(theta, x, y):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4b: Implementing Gradient Descent and Using It to Optimize the Sin Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement gradient descent. \n",
    "\n",
    "Recall that the gradient descent update function follows the form:\n",
    "\n",
    "$$\\large\n",
    "\\vec{\\theta}^{(t+1)} \\leftarrow \\vec{\\theta}^{(t)} - \\alpha \\left (\\nabla_\\vec{\\theta} R(\\vec{\\theta}, \\mathbb{X}, \\vec{y}) \\right )\n",
    "$$\n",
    "where \n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Save the current `theta` in `theta_history`, along with the average squared loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "After completing the function, the cell will output the trajectory from running gradient descent over time.\n",
    "\n",
    "\n",
    "Note that the function you're implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was `gradient_descent(df, initial_guess, alpha, n)`, where `df` was the gradient of the function we are minimizing and `initial_guess` are the starting parameters for that function. Here our signature is a bit different (described below in docstring) than the `gradient_descent` implementation from lecture.\n",
    "\n",
    "**Hints:**\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times. Take a look at `num_iter`.\n",
    "- Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "- Don't forget that `sin_MSE` and `sin_MSE_gradient` require the $x$ and $y$ values to be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.array([0, 0])\n",
    "\n",
    "def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    data -- the data used in the model \n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat, thetas_used, losses_calculated = grad_desc(\n",
    "    sin_MSE, sin_MSE_gradient, theta_start, df, num_iter=20, alpha=0.1\n",
    ")\n",
    "for b, l in zip(thetas_used, losses_calculated):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. The code below plots our $x$-values with our model's predicted $\\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\boldsymbol\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = init_theta()\n",
    "theta_est, thetas, loss = grad_desc(sin_MSE, sin_MSE_gradient, theta_init, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our model output over our observations shows that gradient descent did  a great job finding both the overall increase (slope) of the data, as well as the oscillation frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df['x'], df['y']\n",
    "y_pred = sin_model(x, theta_est)\n",
    "\n",
    "plt.plot(x, y_pred, label='Model ($\\hat{y}$)')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation ($y$)', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Loss\n",
    "\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters.\n",
    "\n",
    "In the previous plot we saw the loss decrease with each iteration. In this part, we'll see the trajectory of the algorithm as it travels the loss surface. Run the following cells to see a visualization of this trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.array(thetas).squeeze()\n",
    "loss = np.array(loss)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "from lab7_utils import plot_3d\n",
    "plot_3d(thetas[:, 0], thetas[:, 1], loss, mean_squared_error, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # A list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # A list or array of theta_2 value\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    theta1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    theta2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(theta1_s, theta2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for theta1, theta2 in data:\n",
    "        l = loss_function(model(x, np.array([theta1, theta2])), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    # Create the contour \n",
    "    theta_points = go.Scatter(name=\"theta Values\", \n",
    "                              x=theta_1_series, \n",
    "                              y=theta_2_series,\n",
    "                              mode=\"lines+markers\")\n",
    "    lr_loss_contours = go.Contour(x=theta1_s, \n",
    "                                  y=theta2_s, \n",
    "                                  z=z, \n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, theta_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Static Learning Rate', thetas, mean_squared_error, sin_model, df[\"x\"], df[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, gradient descent is able to navigate even this fairly complex loss space and find a nice minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "To save some computational time, you will often see two modifications in standard library:\n",
    "\n",
    "* Instead of always making some fix number of iteration, we can terminate early if we determine that `theta` already converages. Convergence is usually defined by $\\theta_{t+1} - \\theta_{t}$ is less than some number, and you can specify this in the `tol` argument in `scipy.optimize.minimize` function. The maximum iteration can be set using the `maxiter` paramter as well.\n",
    "* Instead of calculating the risk by averaing over all data points, we estmate the risk by averaging over some subsample of data points. This is known as stochastic gradient descent and usually denoted as SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Congratulations! You finished Lab 07!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> one_hot_X.shape == (244, 13)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all([np.issubdtype(one_hot_X[column].dtype, np.number) for column in one_hot_X])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(model._thetas) == 13\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(model_l1._thetas) == 13\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_model = MyScipyLinearModel()\n>>> test_model.fit(l1, one_hot_X, tips)\n>>> len(model_l1.predict(one_hot_X)) == 244\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_model = MyScipyLinearModel()\n>>> test_model.fit(l2, one_hot_X, tips)\n>>> len(test_model.predict(one_hot_X)) == 244\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(sklearn_model, LinearRegression)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(sklearn_model.coef_) == 13\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(sklearn_model.predict(one_hot_X)) == 244\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(sklearn_model_intercept.coef_) == 12\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sklearn_model_intercept.intercept_ != 0\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test = np.array([[1, 2], [-1, -9]])\n>>> Y_test = np.array([1, 3])\n>>> model_test = MyAnalyticallyFitOLSModel()\n>>> model_test.fit(X_test, Y_test)\n>>> assert np.all(np.isclose(model_test._thetas, np.array([ 2.14285714, -0.57142857])))\n>>> assert len(model_test.predict(X_test)) == 2\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_test = pd.DataFrame([[1, 2], [-1, -9]])\n>>> Y_test = pd.Series([1, 3])\n>>> model_test = MyAnalyticallyFitOLSModel()\n>>> model_test.fit(X_test, Y_test)\n>>> assert np.all(np.isclose(model_test._thetas, np.array([ 2.14285714, -0.57142857])))\n>>> assert len(model_test.predict(X_test)) == 2\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> one_hot_X_revised.shape == (244, 9)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_squared_error(scipy_model.predict(one_hot_X_revised), tips), 1.043942840612841, rtol=1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_squared_error(analytical_model.predict(one_hot_X_revised), tips), 1.043942840612841, rtol=1)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> x, y = df['x'], df['y']\n>>> np.isclose(sin_MSE([0, np.pi], x, y), 19.49000412080223)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> x, y = df['x'], df['y']\n>>> np.isclose(sin_MSE([0, np.pi/2], x, y), 20.954427404991762)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> x, y = df['x'], df['y']\n>>> np.isclose(sin_MSE_dt1([0, np.pi], x, y), -25.376660670924529)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> x, y = df['x'], df['y']\n>>> np.isclose(sin_MSE_dt1([0, np.pi/2], x, y), -25.815630534245813)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> x, y = df['x'], df['y']\n>>> np.isclose(sin_MSE_dt2([0, np.pi], x, y), 1.9427210155296564)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> x, y = df['x'], df['y']\n>>> np.isclose(sin_MSE_dt2([0, np.pi/2], x, y), -8.680852400281287)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.round(theta_hat[0],8) == 1.51625373 and np.round(theta_hat[1],8) == 2.99448441\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (np.round(thetas_used[1][0],8)) == 2.60105745 and (np.round(thetas_used[1][1],8)) == 2.60105745\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (np.round(thetas_used[3][0],8)) == 2.05633644 and (np.round(thetas_used[3][1],7)) == 2.9631291\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (np.round(losses_calculated[5],8)) == 0.90732714 and (np.round(losses_calculated[8],8)) == 0.29697507\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
