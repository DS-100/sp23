{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw03.ipynb\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 3: Text Analysis Using Twitter\n",
    "\n",
    "## Cleaning and Exploring Twitter Data using REGEX\n",
    "\n",
    "## Due Date: Thursday Feb 9, 11:59 PM\n",
    "You must submit this assignment to Gradescope by the on-time deadline, Thursday, Feburary 9, 11:59pm. Please read the syllabus for the grace period policy. No late\n",
    "submissions beyond the grace period will be accepted. **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance (TAs need to sleep, after all!).\n",
    "\n",
    "Please read the instructions carefully to submit your work to both the coding and written portals of Gradescope.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your notebook."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Collaborators**: *list collaborators here*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This Assignment\n",
    "\n",
    "Welcome to the third homework assignment of Data 100! In this assignment, we will be exploring tweets from several high profile Twitter users.  \n",
    "\n",
    "In this assignment you will gain practice with:\n",
    "* Conducting Data Cleaning and EDA on a text-based dataset.\n",
    "* Manipulating data in pandas with the datetime and string accessors.\n",
    "* Writing regular expressions and using pandas regex methods.\n",
    "* Performing sentiment analysis on social media using VADER.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to set up your notebook.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from ds100_utils import *\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets.\n",
    "pd.set_option('max_colwidth', 280)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "def horiz_concat_df(dict_of_df, head=None):\n",
    "    \"\"\"\n",
    "    Horizontally concatenante multiple DataFrames for easier visualization. \n",
    "    Each DataFrame must have the same columns.\n",
    "    \"\"\"\n",
    "    df = pd.concat([df.reset_index(drop=True) for df in dict_of_df.values()], axis=1, keys=dict_of_df.keys())\n",
    "    if head is None:\n",
    "        return df\n",
    "    return df.head(head)"
   ],
   "outputs": [],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Score Breakdown\n",
    "\n",
    "Question | Manual| Points\n",
    "--- |---| ---\n",
    "1a |No| 1\n",
    "1b |No| 1\n",
    "1c |No| 3\n",
    "1d |Yes| 1\n",
    "2a |No| 2\n",
    "2b |No| 2\n",
    "2c |No| 2\n",
    "2d |No| 2\n",
    "2e |Yes| 2\n",
    "2f |Yes| 1\n",
    "3a |No| 1\n",
    "3b |Yes| 1\n",
    "3c |No| 1\n",
    "4a |Yes| 1\n",
    "4b |No| 1\n",
    "4ci |No| 1\n",
    "4cii |No| 1\n",
    "4d |No| 1\n",
    "4e |No| 2\n",
    "4f |No| 2\n",
    "4g |yes| 2\n",
    "5a |yes| 2\n",
    "5b |Yes| 2\n",
    "**Total** || **35**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Importing the Data\n",
    "\n",
    "\n",
    "The data for this assignment was obtained using the [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api).  To ensure that everyone has the same data and to eliminate the need for every student to apply for a Twitter developer account, we have collected a sample of tweets from several high-profile public figures.  The data is stored in the folder `data`.  Run the following cell to list the contents of the directory:"
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to list the content, no further action is needed.\n",
    "from os import listdir\n",
    "for f in listdir(\"data\"):\n",
    "    print(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "### Question 1a\n",
    "\n",
    "Let's examine the contents of one of these files.  Using the [`open` function](https://docs.python.org/3/library/functions.html#open) and [`read` operation](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects) on a python file object, read the first 1000 **characters** in `data/BernieSanders_recent_tweets.txt` and store your result in the variable `q1a`.  Then display the result so you can read it.\n",
    "\n",
    "**Caution:** Viewing the contents of large files in a Jupyter notebook could crash your browser.  Be careful not to print the entire contents of the file.\n",
    "\n",
    "**Hint1:** You might want to try to use `with`:\n",
    "\n",
    "```python\n",
    "with open(\"filename\", \"r\") as f:\n",
    "    f.read(2)\n",
    "```\n",
    "**Hint2:** Your datapath should start with `data/...`, absolute path will not be accepted.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "...\n",
    "    q1a = ...\n",
    "print(q1a)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q1a\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "### Question 1b\n",
    "\n",
    "What format is the data in? Answer this question by entering the letter corresponding to the right format in the variable `q1b` below.\n",
    "\n",
    "**Caution:** Viewing the contents of large files in a Jupyter notebook could crash your browser.  Be careful not to print the entire contents of the file.\n",
    "\n",
    "  **A.** CSV<br/>\n",
    "  **B.** HTML<br/>\n",
    "  **C.** JavaScript Object Notation (JSON)<br/>\n",
    "  **D.** Excel XML\n",
    "\n",
    "Answer in the following cell. Your answer should be a string, either `\"A\"`, `\"B\"`, `\"C\"`, or `\"D\"`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "q1b = ..."
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q1b\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "\n",
    "### Question 1c\n",
    "\n",
    "Pandas has built-in readers for many different file formats including the file format used here to store tweets.  To learn more about these, check out the documentation for `pd.read_csv` [(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), `pd.read_html`[(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html), `pd.read_json`[(docs)](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html), and `pd.read_excel`[(docs)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html).  \n",
    "\n",
    "1. Use one of these functions to populate the `tweets` dictionary with the tweets for: `AOC`, `Cristiano`, and `elonmusk`. The keys of `tweets` should be the handles of the users, which we have provided in the cell below, and the values should be the DataFrames.\n",
    "2. Set the index of each DataFrame to correspond to the `id` of each tweet.  \n",
    "\n",
    "\n",
    "\n",
    "**Hint:** You might want to first try loading one of the DataFrames before trying to complete the entire question.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = ..."
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q1c\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you did everything correctly, the following cells will show you the first 5 tweets for Elon Musk (and a lot of information about those tweets)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to show the first 5 tweets for Elon Musk, no further action is needed.\n",
    "tweets[\"elonmusk\"].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "There are many ways we could choose to read tweets. Why might someone be interested in doing data analysis on tweets? Name a kind of person or institution which might be interested in this kind of analysis. Then, give two reasons why a data analysis of tweets might be interesting or useful for them. Answer in 2-3 sentences.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 2:  Source Analysis\n",
    "\n",
    "\n",
    "In some cases, the Twitter feed of a public figure may be partially managed by a public relations firm. In these cases, the device used to post the tweet may help reveal whether it was the individual (e.g., from an iPhone) or a public relations firm (e.g., TweetDeck).  The tweets we have collected contain the source information but it is formatted strangely :("
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to see the source column of Cristinao's tweet dataframe, no futher action is needed.\n",
    "tweets[\"Cristiano\"][[\"source\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this question we will use a regular expression to convert this messy HTML snippet into something more readable.  For example: `<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>` should be `Twitter for iPhone`. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "### Question 2a\n",
    "\n",
    "We will first use the Python `re` library to cleanup the above test string.  In the cell below, write a regular expression that will match the **HTML tag** and assign it to the variable `q2a_pattern`. We then use the `re.sub` function to substitute anything that matches the pattern with an empty string `\"\"`.\n",
    "\n",
    "An HTML tag is defined as a `<` character followed by zero or more non-`>` characters, followed by a `>` character. That is, `<a>` and `</a>` are both considered _separate_ HTML tags.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "q2a_pattern = ...\n",
    "test_str = '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n",
    "re.sub(q2a_pattern, \"\", test_str)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q2a\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "--- \n",
    "### Question 2b\n",
    "\n",
    "Rather than writing a regular expression to detect and remove the HTML tags we could instead write a regular expression to **capture** the device name between the angle brackets.  To simplify the problem, you may assume the device name is between a right angle bracket and a left angle bracket. Here we will use [**capturing groups**](https://docs.python.org/3/howto/regex.html#grouping) by placing parenthesis around the part of the regular expression we want to return..  \n",
    "\n",
    "\n",
    "**Hint:** The output of the following cell should be `['Twitter for iPhone']`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "q2b_pattern = ...\n",
    "test_str = '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n",
    "re.findall(q2b_pattern, test_str)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q2b\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 2c\n",
    "\n",
    "Using either of the two regular expressions you just created and `Series.str.replace`[(docs)](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html) or `Series.str.extract`[(docs)](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html), add a new column called `\"device\"` to **all** of the DataFrames in `tweets` containing just the text describing the device (without the HTML tags). You may use one or multiple lines.\n",
    "\n",
    "**Note:** You do not need to follow the provided skeleton code. You may also find `DataFrame.assign` helpful, see the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html), but it is not required."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweets = ..."
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q2c\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 2d\n",
    "\n",
    "To examine the most frequently used devices by each individual, implement the `most_freq` function that takes in a `Series` and returns a new `Series` containing the `k` most commonly occurring entries in the first series, where the values are the counts of the entries and the indices are the entries themselves.\n",
    "\n",
    "For example: \n",
    "```python\n",
    "most_freq(pd.Series([\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"]), k=2)\n",
    "```\n",
    "would return:\n",
    "```\n",
    "A    3\n",
    "B    2\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Hint**: Consider using `value_counts`, `sort_values`, `head`, and/or `nlargest` (for the last one, read the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.nlargest.html?highlight=nlargest)).\n",
    " Think of what might be the most efficient implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def most_freq(series, k = 5):\n",
    "    ...\n",
    "\n",
    "most_freq(tweets[\"Cristiano\"]['device'])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q2d\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following two cells to compute a table and plot describing the top 5 most commonly used devices for each user."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to compute a table, no further action needed.\n",
    "device_counts = pd.DataFrame(\n",
    "    [most_freq(tweets[name]['device']).rename(name)\n",
    "     for name in tweets]\n",
    ").fillna(0)\n",
    "device_counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to generate the plot, no further action needed.\n",
    "make_bar_plot(device_counts.T, title=\"Count of Tweets by Source\",\n",
    "               xlabel=\"Source\", ylabel=\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Handle\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 2e\n",
    "\n",
    "What might we want to investigate further based on the plot in 2d above?  Write a few sentences below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 2f\n",
    "\n",
    "We just looked at the top 5 most commonly used devices for each user. However, we used the number of tweets as a measure, when it might be better to compare these distributions by comparing _proportions_ of tweets. Why might proportions of tweets be better measures than numbers of tweets?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 3: When?\n",
    "\n",
    "Now that we've explored the sources of each of the tweets, we will perform some time series analysis. A look into the temporal aspect of the data could reveal insights about how a user spends their day, when they eat and sleep, etc. In this question, we will focus on the time at which each tweet was posted.\n"
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3a\n",
    "\n",
    "Complete the following function `add_hour` that takes in a tweets dataframe `df`, and two column names `time_col` and `result_col`. Your function should use the timestamps in the `time_col` column to store in a new column `result_col` the computed  hour of the day as a floating point number according to the formula:\n",
    "\n",
    "$$\n",
    "\\text{hour} + \\frac{\\text{minute}}{60} + \\frac{\\text{second}}{60^{2}}\n",
    "$$\n",
    "\n",
    "**Note:** The below code calls your `add_hour` function and updates each tweets dataframe by using the `created_at` timestamp column to calculate and store the `hour` column.\n",
    "\n",
    "**Hint:** See the following link for an example of working with timestamps using the [`dt` accessors](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor). \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def add_hour(df, time_col, result_col):\n",
    "    ...\n",
    "    return df\n",
    "\n",
    "# Do not modify the below code.\n",
    "tweets = {handle: add_hour(df, \"created_at\", \"hour\") for handle, df in tweets.items()}\n",
    "tweets[\"AOC\"][\"hour\"].head()"
   ],
   "outputs": [],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q3a\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our new `hour` column, let's take a look at the distribution of tweets for each user by time of day. The following cell helps create a density plot on the number of tweets based on the hour they are posted. \n",
    "\n",
    "The function `bin_df` takes in a dataframe, an array of bins, and a column name; it bins the the values in the specified column, returning a dataframe with the bin lower bound and the number of elements in the bin. This function uses [`pd.cut`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html), a pandas [utility](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) for binning numerical values that you may find helpful in the distant future.\n",
    "\n",
    "Run the cell and answer the following question about the plot."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to genertae the plot, no further action is needed.\n",
    "def bin_df(df, bins, colname):\n",
    "    binned = pd.cut(df[colname], bins).value_counts().sort_index()\n",
    "    return pd.DataFrame({\"counts\": binned, \"bin\": bins[:-1]})\n",
    "\n",
    "hour_bins = np.arange(0, 24.5, .5)\n",
    "binned_hours = {handle: bin_df(df, hour_bins, \"hour\") for handle, df in tweets.items()}\n",
    "\n",
    "make_line_plot(binned_hours, \"bin\", \"counts\", title=\"Distribution of Tweets by Time of Day\",\n",
    "               xlabel=\"Hour\", ylabel=\"Number of Tweets\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3b\n",
    "Compare Cristiano's distribution with those of AOC and Elon Musk. In particular, compare the distributions before and after Hour 6. What differences did you notice? What might be a possible cause of that? Do the data plotted above seem reasonable?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 3c\n",
    "\n",
    "To account for different locations of each user in our analysis, we will next adjust the `created_at` timestamp for each tweet to the respective timezone of each user. Complete the following function `convert_timezone` that takes in a tweets dataframe `df` and a timezone `new_tz` and adds a new column `converted_time` that has the adjusted `created_at` timestamp for each tweet. The timezone for each user is provided in `timezones`.\n",
    "\n",
    "**Hint:** Again, please see the following link for an example of working with [`dt` accessors](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_timezone(df, new_tz):\n",
    "    ...\n",
    "    return df\n",
    "\n",
    "timezones = {\"AOC\": \"EST\", \"Cristiano\": \"Europe/Lisbon\", \"elonmusk\": \"America/Los_Angeles\"}\n",
    "\n",
    "tweets = {handle: convert_timezone(df, tz) for (handle, df), tz in zip(tweets.items(), timezones.values())}"
   ],
   "outputs": [],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "convert-to-est",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q3c\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our adjusted timestamps for each user based on their timezone, let's take a look again at the distribution of tweets by time of day."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to generate the plot, no further action is needed.\n",
    "tweets = {handle: add_hour(df, \"converted_time\", \"converted_hour\") for handle, df in tweets.items()}\n",
    "binned_hours = {handle: bin_df(df, hour_bins, \"converted_hour\") for handle, df in tweets.items()}\n",
    "\n",
    "make_line_plot(binned_hours, \"bin\", \"counts\", title=\"Distribution of Tweets by Time of Day (timezone-corrected)\",\n",
    "               xlabel=\"Hour\", ylabel=\"Number of Tweets\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 4: Sentiment Analysis\n",
    "\n",
    "\n",
    "In the past few questions, we have explored the sources of the tweets and when they are posted. Although on their own, they might not seem particularly intricate, combined with the power of regular expressions, they could actually help us infer a lot about the users. In this section, we will continue building on our past analysis and specifically look at the **sentiment of each tweet** -- this would lead us to a much more direct and detailed understanding of how users view certain subjects and people. **Sentiment analysis** is generally the computational task of classifying the emotions in a body of text as positively or negatively charged."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "How do we actually measure the sentiment of each tweet? In our case, we can use the words in the text of a tweet for our calculation! For example, the word \"love\" within the sentence \"I love America!\" has a positive sentiment, whereas the word \"hate\" within the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon ([github](https://github.com/cjhutto/vaderSentiment), [original paper](https://doi.org/10.1609/icwsm.v8i1.14550)) to analyze the sentiment of AOC's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to print the first 10 rows, no further action needed.\n",
    "print(''.join(open(\"vader_lexicon.txt\").readlines()[:10]))"
   ],
   "outputs": [],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "head-vader",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the lexicon contains emojis too! Each row contains a word (\"token\") and various measures of the **polarity** of that word, measuring how positive or negative the word is, on a scale of -4 (extremely negative) to +4 (extremely positive). We explain more below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VADER Sentiment Analysis\n",
    "\n",
    "VADER ([github](https://github.com/cjhutto/vaderSentiment), [original paper](https://doi.org/10.1609/icwsm.v8i1.14550)) is a tool that can quantitatively describe the polarity or \"sentiment\" of a word.\n",
    "\n",
    "VADER doesn't \"read\" sentences, but works by parsing sentences into words, assigning a preset generalized score from their testing sets to each word separately. \n",
    "\n",
    "VADER relies on humans to stabilize its scoring. The creators use Amazon Mechanical Turk, a crowdsourcing survey platform, to train its model. Its training data consists of a small corpus of tweets, New York Times editorials and news articles, Rotten Tomatoes reviews, and Amazon product reviews, tokenized using the natural language toolkit (NLTK). Each word in each dataset was reviewed and rated on a scale of -4 (extremely negative) to 4 (extremely positive) by at least 10 trained individuals who had signed up to work on these tasks through Mechanical Turk. \n"
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6a-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4a\n",
    "Please score the sentiment of one of the following words, using your own personal interpretation. No code is required for this question!\n",
    "\n",
    "- police\n",
    "- order\n",
    "- Democrat\n",
    "- Republican\n",
    "- gun\n",
    "- dog\n",
    "- technology\n",
    "- TikTok\n",
    "- security\n",
    "- face-mask\n",
    "- science\n",
    "- climate change\n",
    "- vaccine\n",
    "\n",
    "What score did you give it and why? Can you think of a situation in which this word would carry the opposite sentiment to the one youâ€™ve just assigned?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**Optional (ungraded):** Are there circumstances (e.g. certain kinds of language or data) when you might not want to use VADER? What features of human speech might VADER misrepresent or fail to capture?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4b\n",
    "\n",
    "Let's first load in the data containing all the sentiments. \n",
    "In `vader_lexicon.txt`, each row contains the word (token), average polarity, standard deviation of polarity, and the \"raw polarity ratings\" of each of the 10 human raters.\n",
    "\n",
    "Read `vader_lexicon.txt` into a new dataframe called `sent`. The index of the dataframe should be the words in the lexicon and should be named `token`. `sent` should have one column named `polarity`, storing the average polarity of each word. We will not incorporate the polarity standard deviation and raw ratings for this exercise.\n",
    "\n",
    "**Hint1:** The `pd.read_csv` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)) function may help here. \n",
    "\n",
    "**Hint2:** Since the file is tab-separated, be sure to read the documentation on how to set the separator with `pd.read_csv`'s parameter `sep`. To check you work, the first token should be `$:`.\n",
    "\n",
    "**Hint3:** Is there a header (that is, data that can be used as column names) in the csv file and how can you account for this?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent = ...\n",
    "sent.head()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q4b\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4c\n",
    "\n",
    "Before further analysis, we will need some more tools that can help us extract the necessary information and clean our data.\n",
    "\n",
    "Complete the following regular expressions that will help us match part of a tweet that we either (i) want to remove or (ii) are interested in learning more about."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6b-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Question 4c Part i**\n",
    "Assign a regular expression to a new variable `punct_re` that captures all of the punctuations within a tweet. We consider punctuation to be any non-word, non-whitespace character.\n",
    "\n",
    "**Note**: A word character is any character that is alphanumeric or an underscore. A whitespace character is any character that is a space, a tab, a new line, or a carriage return.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "punct_re = ...\n",
    "\n",
    "re.sub(punct_re, \" \", tweets[\"AOC\"].iloc[0][\"full_text\"])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q4ci\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Question 4c Part ii**\n",
    "Assign a regular expression to a new variable `mentions_re` that matches any mention in a tweet. Your regular expression should use a capturing group to extract the user's username in a mention.\n",
    "\n",
    "**Hint**: a user mention within a tweet always starts with the `@` symbol and is followed by a series of word characters (with no space in between). For more explanations on what a word character is, check out the **Note** section in Part 1.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mentions_re = ...\n",
    "\n",
    "re.findall(mentions_re, tweets[\"AOC\"].iloc[0][\"full_text\"])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q4cii\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "\n",
    "### Tweet Sentiments and User Mentions\n",
    "\n",
    "As you have seen in the previous part of this question, there are actually a lot of interesting components that we can extract out of a tweet for further analysis! For the rest of this question though, we will focus on one particular case: the sentiment of each tweet in relation to the users mentioned within it. \n",
    "\n",
    "To calculate the sentiments for a sentence, we will follow this procedure:\n",
    "\n",
    "1. Remove the punctuation from each tweet so we can analyze the words.\n",
    "2. For each tweet, find the sentiment of each word.\n",
    "3. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4d\n",
    "\n",
    "Let's use our `punct_re` regular expression from the previous part to clean up the text a bit more! The goal here is to remove all of the punctuations to ensure words can be properly matched with those from VADER to actually calculate the full sentiment score.\n",
    "\n",
    "Complete the following function `sanitize_texts` that takes in a table `df` and adds a new column `clean_text` by converting all characters in its original `full_text` column to lower case and replace all instances of punctuations with a space character.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sanitize_texts(df):\n",
    "    ...\n",
    "    return df\n",
    "\n",
    "tweets = {handle: sanitize_texts(df) for handle, df in tweets.items()}\n",
    "tweets[\"AOC\"][\"clean_text\"].head()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q4d\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4e\n",
    "With the texts sanitized, we can now extract all the user mentions from tweets. \n",
    "\n",
    "Complete the following function `extract_mentions` that takes in the **`full_text`** (not `clean_text`!) column from a tweets dataframe  and uses `mentions_re` to extract all the mentions in a dataframe. The returned dataframe is:\n",
    "* single-indexed by the IDs of the tweets\n",
    "* has one row for each mention\n",
    "* has one column named `mentions`, which contains each mention in all lower-cased characters\n",
    "\n",
    "\n",
    "**Note:** While staff solution contains a single line, you may find it helpful to break the problem into multiple subparts and complete this in multiple lines.\n",
    "\n",
    "**Hints**: There are several ways to approach this problem.\n",
    "* Here is a list of documentations for potentially useful function. You are not expected to use all of them, you may also need additional functions that are not listed below.\n",
    "    * Extracting valid mentions: `str.extractall` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extractall.html?highlight=extractall)), `str.findall` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.findall.html)), `dropna` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dropna.html))\n",
    "    * Refomatting data: `.reset_index` ([link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html)), `.explode` ([link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html)), `.to_frame`([link](https://pandas.pydata.org/docs/reference/api/pandas.Series.to_frame.html)).\n",
    "    * You can find an example of how to chain `.explode` and `.to_frame` in the *Tidying Up the Data* section following this question.\n",
    "* The staff solution uses a single line of chained methods, but you are encouraged to break the problem into subparts with multiple lines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_mentions(full_texts):\n",
    "    mentions = ...\n",
    "    return mentions[[\"mentions\"]]\n",
    "\n",
    "# Uncomment this line to help you debug.\n",
    "display(extract_mentions(tweets[\"AOC\"][\"full_text\"]).head())\n",
    "\n",
    "# Do not modify the below code.\n",
    "mentions = {handle: extract_mentions(df[\"full_text\"]) for handle, df in tweets.items()}\n",
    "horiz_concat_df(mentions).head()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q4e\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "\n",
    "### Tidying Up the Data\n",
    "\n",
    "Now, let's convert the tweets into what's called a [*tidy format*](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to make the sentiments easier to calculate. The `to_tidy_format` function implemented for you uses the `clean_text` column of each tweets dataframe to create a tidy table, which is:\n",
    "\n",
    "* single-indexed by the IDs of the tweets, for every word in the tweet.\n",
    "* has one column named `word`, which contains the individual words of each tweet.\n",
    "\n",
    "Run the following cell to convert the table into the tidy format. Take a look at the first 5 rows from the \"tidied\" tweets dataframe for AOC and see if you can find out how the structure has changed.\n",
    "\n",
    "**Note**: Although there is no work needed on your part, we have referenced a few more advanced pandas methods you might have not seen before -- you should definitely look them up in the documentation when you have a chance, as they are quite powerful in restructuring a dataframe into a useful intermediate state!"
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6d-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run this cell to convert table into tidy format, no further action is needed.\n",
    "def to_tidy_format(df):\n",
    "    tidy = (\n",
    "        df[\"clean_text\"]\n",
    "        .str.split()\n",
    "        .explode()\n",
    "        .to_frame()\n",
    "        .rename(columns={\"clean_text\": \"word\"})\n",
    "    )\n",
    "    return tidy\n",
    "\n",
    "tidy_tweets = {handle: to_tidy_format(df) for handle, df in tweets.items()}\n",
    "tidy_tweets[\"AOC\"].head()"
   ],
   "outputs": [],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding in the Polarity Score\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. \n",
    "\n",
    "The following `add_polarity` function adds a new `polarity` column to the `df` table. The `polarity` column contains the sum of the sentiment polarity of each word in the text of the tweet.\n",
    "\n",
    "**Note**: Again, though there is no work needed on your part, it is important for you to go through how we set up this method and actually understand what each method is doing. In particular, see how we deal with missing data."
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6e-header",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Just run this cell to add the \"polarity\" column.\n",
    "# No further code is needed, but verify your understanding of each chained method.\n",
    "def add_polarity(df, tidy_df):\n",
    "    df[\"polarity\"] = (\n",
    "        tidy_df\n",
    "        .merge(sent, how='left', left_on='word', right_index=True)\n",
    "        .reset_index()\n",
    "        .loc[:, ['id', 'polarity']]\n",
    "        .fillna(0)\n",
    "        .groupby('id')\n",
    "        .sum()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "tweets = {handle: add_polarity(df, tidy_df) for (handle, df), tidy_df in \\\n",
    "          zip(tweets.items(), tidy_tweets.values())}\n",
    "tweets[\"AOC\"][[\"clean_text\", \"polarity\"]].head()"
   ],
   "outputs": [],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q6e",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comment: In the demo cell above, `add_polarity()` is a very straightforward approach to sentiment analysis: define a tweet's sentiment as the **sum** of each word's sentiment as determined by a VADER lexicon. The VADER lexicon itself relies on crowdsourcing humans to stabilize its scoring. However, sentence structure and word phrasing heavily impacts sentiment, but our current approach ignores these contexts, instead opting for approximate, naive sentiments to perform initial EDA. \n",
    "\n",
    "If we were to further explore this direction of the data, we would consider approaches for computing tweet sentiment that are modern, nuanced, and more accurate (for some definition of \"accurate\"). Such approaches often adapt deep natural language processing models to sentiment analysis tasks, meaning they directly address the sentiment of a body of text, instead of individual words like VADER. However, these models still depend on a robust \"training dataset\" of tweet sentiments, which is often still generated through crowdsourced human work. If you're curious about this, explore Data C104: Human Contexts and Ethics and CS 288: Natural Language Processing!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4f\n",
    "Finally, with our polarity column in place, we can finally explore how the sentiment of each tweet relates to the user(s) mentioned in it. \n",
    "\n",
    "Complete the following function `mention_polarity` that takes in a mentions dataframe `mentions` and the original tweets dataframe `df` and returns a series where the mentioned users are the index and the corresponding mean sentiment scores of the tweets mentioning them are the values.\n",
    "\n",
    "**Hint**: You should consider joining tables together in this question.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mention_polarity(df, mention_df):\n",
    "    ...\n",
    "\n",
    "aoc_mention_polarity = mention_polarity(tweets[\"AOC\"],mentions[\"AOC\"]).sort_values(ascending=False)\n",
    "aoc_mention_polarity"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grader.check(\"q4f\")"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 4g\n",
    "\n",
    "When grouping by mentions and aggregating the polarity of the tweets, what aggregation function should we use? What might be one drawback of using the mean?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Type your answer here, replacing this text._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "## Question 5: You Do EDA!\n",
    "\n",
    "Congratulations! You have finished all of the preliminary analysis on AOC, Cristiano, and Elon Musk's recent tweets. \n",
    "\n",
    "As you might have recognized, there is still far more to explore within the data and build upon what we have uncovered so far. In this open-ended question, we want you to come up with a new perspective that can expand upon our analysis of the sentiment of each tweet. \n",
    "\n",
    "For this question, you will perform some text analysis on our `tweets` dataset. Your analysis should have two parts:\n",
    "\n",
    "1. a piece of code that manipulates `tweets` in some way and produces informative output (e.g. a dataframe, series, or plot)\n",
    "2. a short (4-5 sentence) description of the findings of your analysis: what were you looking for? What did you find? How did you go about answering your question?\n",
    "\n",
    "Your work should involve text analysis in some way, whether that's using regular expressions or some other form.\n",
    "\n",
    "To aid you in creating plots, we provide the plotting helper functions in the table below. These are same helpers we have used throughout this notebook, and all accept dictionaries with a similar structure to `tweets`. That being said, if you know how to make plots, please do so! Very soon in this class, you'll learn how to use the matplotlib and seaborn libraries that we use to write these the helpers.\n",
    "\n",
    "| Helper | Description |\n",
    "|--------|-------------|\n",
    "| `make_bar_plot` | Plot side-by-side bar plots of data like [`plt.bar`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.bar.html) |\n",
    "| `make_histogram` | Plot overlaid histograms of data like [`plt.hist`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) |\n",
    "| `make_line_plot` | Plot overlaid line plots of data like [`plt.plot`](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.plot.html) |\n",
    "| `make_scatter_plot` | Plot overlaid scatter plots of data like [`plt.scatter`](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.scatter.html) |\n",
    "\n",
    "Each of the provided helpers is in `ds100_utils.py` and has a comprehensive docstring. You can read the docstring by calling `help` on the plotting function:"
   ],
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "help(make_line_plot)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To assist you in getting started, here are a few ideas for this you can analyze for this question:\n",
    "\n",
    "- dig deeper into when devices were used\n",
    "- how sentiment varies with time of tweet\n",
    "- expand on regexes from 4b to perform additional analysis (e.g. hashtags)\n",
    "- examine sentiment of tweets over time\n",
    "\n",
    "In general, try to combine the analyses from earlier questions or create new analysis based on the scaffolding we have provided.\n",
    "\n",
    "This question is worth 4 points and will be graded based on this rubric:\n",
    "\n",
    "| | 2 points | 1 point | 0 points |\n",
    "|-----|-----|-----|-----|\n",
    "| **Code** | Produces a mostly informative plot or pandas output that addresses the question posed in the student's description and uses at least one of the following pandas DataFrame/Series methods: `groupby`, `agg`, `merge`, `pivot_table`, `str`, `apply` | Attempts to produce a plot or manipulate data but the output is unrelated to the proposed question, or doesn't utilize at least one of the listed methods | No attempt at writing code |\n",
    "| **Description** | Describes the analysis question and procedure comprehensively and summarizes results correctly | Attempts to describe analysis and results but description of results is incorrect or analysis of results is disconnected from the studentâ€™s original question | No attempt at writing a description |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 5a\n",
    "\n",
    "Use this space to put your EDA code.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# perform your text analysis here"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "### Question 5b\n",
    "\n",
    "Use this space to put your EDA description.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Write your description here._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You have finished Homework 3!\n",
    "Below, you will see two cells. Running the first cell will automatically generate a PDF of all questions that need to be manually graded, and running the second cell will automatically generate a zip with your autograded answers. **You are responsible for both the coding portion (the zip from Homework 3) and the written portion (the PDF with from Homework 3) to their respective Gradescope portals.** The coding proportion should be submitted to Homework 2 Coding as a single zip file, and the written portion should be submitted to Homework 3 Written as a single pdf file. When submitting the written portion, please ensure you select pages appropriately. \n",
    "\n",
    "If there are issues with automatically generating the PDF in the first cell, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. Either way, **you are responsible for ensuring your submission follows our requirements, we will NOT be granting regrade requests for submissions that don't follow instructions.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"hw03.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('hw03.pdf')):\n",
    "    display(HTML(\"Download your PDF <a href='hw03.pdf' download>here</a>.\"))\n",
    "else:\n",
    "    print(\"\\n Pdf generation fails, please try the other methods described above\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1a) == 1000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q1a.startswith(\"[{\\\"created_at\\\":\")\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> \"\\\"user\\\": {\\\"id\\\": 216776631\" in q1a\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q1b.upper() in ['A', 'B', 'C', 'D']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert set(tweets.keys()) == {\"AOC\", \"Cristiano\", \"elonmusk\"}\n>>> assert all(df.index.name == \"id\" for df in tweets.values())\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> expected_cols = ['created_at', 'id_str', 'full_text', 'truncated', 'display_text_range',\n...        'entities', 'source', 'in_reply_to_status_id',\n...        'in_reply_to_status_id_str', 'in_reply_to_user_id',\n...        'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo',\n...        'coordinates', 'place', 'contributors', 'retweeted_status',\n...        'is_quote_status', 'retweet_count', 'favorite_count', 'favorited',\n...        'retweeted', 'lang', 'possibly_sensitive', 'extended_entities',\n...        'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',\n...        'quoted_status']\n>>> all(col in df.columns for df in tweets.values() for col in expected_cols)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (tweets[\"AOC\"].shape[0] is not None) and (tweets[\"AOC\"].shape[0] >= 30)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (tweets[\"Cristiano\"].shape[0] is not None) and (tweets[\"Cristiano\"].shape[0] >= 30)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (tweets[\"elonmusk\"].shape[0] is not None) and (tweets[\"elonmusk\"].shape[0] >= 30)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(42)\n>>> exp = set([1333958991613923331,\n...  1312936075590074368,\n...  1248798917350883337,\n...  1204575943601328128,\n...  1346163629486387201,\n...  1246500923180036096,\n...  1315825309036564482,\n...  1255121845533229056,\n...  1352691282603364362,\n...  1243377508835381249])\n>>> set(np.random.choice(sorted(tweets[\"AOC\"].index), replace=False, size=10)) == exp\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(42)\n>>> exp = set([880473340619759616,\n...  174892632500219905,\n...  284705925351239680,\n...  403528698936041473,\n...  476770924587667456,\n...  697801434755133440,\n...  1214232898917801984,\n...  741648660476383233,\n...  48000397721337856,\n...  95524319232409600])\n>>> set(np.random.choice(sorted(tweets[\"Cristiano\"].index), replace=False, size=10)) == exp\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(42)\n>>> exp = set([1265750340613427200,\n...  1321275062998257665,\n...  1285430635088076800,\n...  1252987963329388544,\n...  1339567835840962560,\n...  1313512170336944128,\n...  1348823685445136385,\n...  1280090680430178305,\n...  1301647642846547971,\n...  1281695247626416129])\n>>> set(np.random.choice(sorted(tweets[\"elonmusk\"].index), replace=False, size=10)) == exp\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> re.sub(q2a_pattern, \"\", test_str) == 'Twitter for iPhone'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.sub(q2a_pattern, \"\", \"<a bad='html'\") == \"<a bad='html'\"\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> re.findall(q2b_pattern, test_str) == ['Twitter for iPhone']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(q2b_pattern, \">findme<\") == ['findme']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(tweets.keys()) == {\"AOC\", \"Cristiano\", 'elonmusk'}\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(\"device\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> counts = tweets[\"AOC\"][\"device\"].value_counts().to_dict()\n>>> (set(counts.keys()) == {'Twitter Media Studio', 'Twitter for iPhone'}) and (set(counts.values()) == {2, 3245})\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.random.seed(10202)\n>>> expected = ['Twitter for iPhone'] * 10\n>>> actual = []\n>>> for i in np.random.choice(sorted(tweets[\"AOC\"].index), replace=False, size=10):\n...     actual.append(tweets[\"AOC\"].loc[i, \"device\"])\n>>> actual == expected\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> most_freq(tweets[\"Cristiano\"]['device'])[\"Twitter for iPhone\"] == 1183\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> most_freq(tweets[\"AOC\"]['device'], k=1)[\"Twitter for iPhone\"] == 3245\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all(\"hour\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(0 <= df[\"hour\"].min() and 24 >= df[\"hour\"].max() for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> all(\"converted_time\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((df[\"converted_time\"] == df[\"created_at\"]).all() for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(df[\"converted_time\"].dt.tz != df[\"created_at\"].dt.tz for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(sent, pd.DataFrame)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sent.shape == (7517, 1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> tuple(sent.columns)  == (\"polarity\", )\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sent.index.name == \"token\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(sent.index[5000:5005]) == ['paranoids', 'pardon', 'pardoned', 'pardoning', 'pardons']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(sent['polarity'].head(), [-1.5, -0.4, -1.5, -0.4, -0.7])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4ci": {
     "name": "q4ci",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(punct_re, str)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.sub(punct_re, \" \", \"a.b.c.1!2?3\")\n'a b c 1 2 3'",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.sub(punct_re, \" \", \"<a href='https://google.com'>Google</a>\")\n' a href  https   google com  Google  a '",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.sub(punct_re, \"!!\", \"some.weird!sentence--?()JDJGK#$&*^\")\n'some!!weird!!sentence!!!!!!!!!!JDJGK!!!!!!!!!!'",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4cii": {
     "name": "q4cii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> re.findall(mentions_re, \"@someone: this regex stuff is cool\") == ['someone']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(mentions_re, tweets[\"AOC\"].loc[1358149122264563712][\"full_text\"])[0] == \"RepEscobar\"\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(mentions_re, \"an empty tweet\") == []\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> re.findall(mentions_re, tweets[\"AOC\"].loc[1181804625588051968][\"full_text\"]) == ['LeanInOrg', 'AOC']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> df = pd.DataFrame({\"full_text\": [\"a clean tweet\", \"an UPPPERcAsE tweet\", \"a ! tweet!!with..(*UF)punctuation\"]})\n>>> df = sanitize_texts(df)\n>>> df[\"clean_text\"].tolist() == ['a clean tweet', 'an upppercase tweet', 'a   tweet  with    uf punctuation']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(\"clean_text\" in df.columns for df in tweets.values())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4e": {
     "name": "q4e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert all(not isinstance(df.index, pd.MultiIndex) for df in mentions.values())\n>>> assert all(set(df.columns) == {\"mentions\"} for df in mentions.values())\n>>> assert all(not df.isnull().values.any() for df in mentions.values())\n>>> assert set(mentions.keys()) == {\"AOC\", \"Cristiano\", \"elonmusk\"}\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> horiz_mentions = horiz_concat_df(mentions)\n>>> horiz_mentions.loc[0][\"AOC\"][\"mentions\"] == 'repescobar'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> horiz_mentions = horiz_concat_df(mentions)\n>>> list(sorted(horiz_mentions.columns)) == [('AOC', 'mentions'), ('Cristiano', 'mentions'), ('elonmusk', 'mentions')]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4f": {
     "name": "q4f",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(aoc_mention_polarity.index) == set(mentions[\"AOC\"][\"mentions\"])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 0 <= aoc_mention_polarity.mean() <= 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}